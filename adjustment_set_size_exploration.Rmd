---
title: "Does the size of the adjustment set follows the same variance-bias trade off as in classic ML?"
author: "Iyar Lin"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F, cache = F)
set.seed(1)
options(scipen = 999)

packages <- c(
  "devtools", # install_github
  "tidyverse", # best thing that happend to me
  "pander" # table rendering
)

sapply(
  packages,
  function(x) if (!require(x, character.only = TRUE, quietly = T)) {
      install.packages(x, quiet = T, verbose = F)
      library(x, character.only = T, quietly = T, verbose = F)
    }
)

if (!require(dagitty)) {
  install_github("jtextor/dagitty/r") # Analysis of structural causal models
  library(dagitty, quietly = T)
}
```

# Does the size of the adjustment set follows the same variance-bias trade off as in classic ML?

Let's look at the DAG below:

```{r define model 2}
g <- dagitty("dag {
Y [outcome]
X [exposure]
X -> Y [beta = 0.2]
Z1 -> X [beta = 0.1]
Z1 -> Z2 [beta = -0.1]
Z2 -> X [beta = 0.3]
Z2 -> Y [beta = 0.05]
Z3 -> Z2 [beta = -0.05]
Z3 -> Y [beta = -0.1]
}")

plot(graphLayout(g))
```

If we're interesting in estimating $\mathbb{E}(Y|do(X=x)$ there's several confounding paths that we need to block.

The exact possible adjustment sets are:

```{r print adjustment sets2}
M = 1000
print(adjustmentSets(g, type = "all"))
```

We can see that we can either control for 2 or 3 out of the confounding variables. Assuming we can measure them all, and we know from the graph they are all "relevant" (e.g. not white noise) does it make sense to use all 3 instead of just a subset?

Below I simulate `r M` dataset for a grid of sample size N and compare the standard deviation of the coefficient estimate as well as the R-squered on a hold out test set:

```{r simulate model 2 data}
N_grid <- seq(4, 40, 10)
model_performance <- data.frame(model = rep(c("small", "large"), length(N_grid)), 
                      N = rep(N_grid, each = 2), 
                      sd = NA, 
                      R_2 = NA)

for(n in N_grid){
  sim_data <- replicate(M, simulateSEM(g, N = n), simplify = F)
  sim_test_data <- replicate(M, simulateSEM(g, N = n), simplify = F)
  sim_small_model <- lapply(sim_data, function(data) lm(Y ~ X + Z1 + Z2, data = data))
  sim_large_model <- lapply(sim_data, function(data) lm(Y ~ X + Z1 + Z2 + Z3, data = data))
  
  model_performance$sd[model_performance$N == n & model_performance$model == "small"] <- sd(sapply(sim_small_model, function(model) coef(model)[2]))
  model_performance$sd[model_performance$N == n & model_performance$model == "large"] <- sd(sapply(sim_large_model, function(model) coef(model)[2]))
  
  model_performance$R_2[model_performance$N == n & model_performance$model == "small"] <- mean(mapply(function(data, model){
    sqrt(mean((predict(model, data) - data$Y)^2))
  }, 
  data = sim_test_data, model = sim_small_model))
  
  model_performance$R_2[model_performance$N == n & model_performance$model == "large"] <- mean(mapply(function(data, model){
    sqrt(mean((predict(model, data) - data$Y)^2))
  }, 
  data = sim_test_data, model = sim_large_model))
  
}

```

Below are the sd results:

```{r print sd results}
model_performance %>% ggplot(aes(N, log(sd), color = model)) + geom_line()
```

Below are the R-squared results:

```{r print R-squared results}
model_performance %>% ggplot(aes(N, log(R_2), color = model)) + geom_line()
```

Oddly enough it looks like if anything, using less variables is better!
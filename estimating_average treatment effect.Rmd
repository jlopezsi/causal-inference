---
title: "Estimating average treatment effect"
author: "Iyar Lin"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F, cache = F, tidy = T)
set.seed(1)
options(scipen = 999)

packages <- c(
  "devtools", # install_github
  "tidyverse", # best thing that happend to me
  "pander", # table rendering
  "rpart" # rpart function
)

sapply(
  packages,
  function(x) if (!require(x, character.only = TRUE, quietly = T)) {
      install.packages(x, quiet = T, verbose = F)
      library(x, character.only = T, quietly = T, verbose = F)
    }
)

if (!require(dagitty)) {
  install_github("jtextor/dagitty/r") # Analysis of structural causal models
  library(dagitty, quietly = T)
}
```

# Intro  
Assuming we're able to find the DAG underlying the dataset, this script demonstrates average treatment effect (ATE) estimation using the backdoor criteria.

# Define model and simulate dataset

Below I plot a graph model with the following variable set:

$$V = \{X, Y, Z, W\}$$

and function set (All disturbances $U$ are distributed standard normal unless stated otherwise):

$$F = \{f_1, f_2, f_3, f_4\}$$

such that 

$$  
X = f_1(Z, U_X) = 
     \begin{cases}
       \text{a,} &\quad\text{if} \, Z + U_X \ge 0.61 \\
       \text{b,} &\quad\text{if} \, Z + U_X \ge -0.61 \, \& \, Z + U_X < 0.61\\
       \text{c,} &\quad\text{if} \, Z + U_X < -0.61\\
     \end{cases}
$$

$$Z = f_3(U_Z) = U_Z$$

$$  
W = f_4(X,U_W) \sim 
     \begin{cases}
       poiss(1) &\quad\text{if} \, X=a \\
       poiss(2) &\quad\text{if} \, X=b \\
       poiss(3) &\quad\text{if} \, X=c\\
     \end{cases}
$$

and 

$$  
Y = f_2(W, Z, U_Y) \sim N(Z + W, 1)
$$

So finally we have 

$$
E(Y|do(X=x)) = 
     \begin{cases}
       1, &\quad\text{if} \, X = a \\
       2, &\quad\text{if} \, X = b \\
       3, &\quad\text{if} \, X = c\\
     \end{cases}
$$
Below is a plot of the resulting DAG:

```{r plot model graph}
g <- dagitty("dag {
Y [outcome]
X [exposure]
X -> W
W -> Y
Z -> X
Z -> Y
}")

plot(graphLayout(g))
```

Below I simulate a dataset according to the above toy model.

```{r simulate data}
N <- 1000
Z <- rnorm(N)
X_tag <- Z + rnorm(N)
X <- ifelse(X_tag > qnorm(2/3, 0, sqrt(2)), "a", 
            ifelse(X_tag > qnorm(1/3, 0, sqrt(2)), "b", "c"))

W <- sapply(X, function(x){
  if(x == "a") rpois(1, 1) 
  else if(x == "b") rpois(1, 2)
  else rpois(1, 3)
})

Y <- mapply(function(z,w){ 
  rnorm(1, z + w, 1)
  }, Z, W)

sim_data <- data.frame(X,Y,Z,W)
```

# Estimate average treatment effect (ATE)

In Pearl eq 3.5 (p. 57) the post intervention of $Y$ given $do(X=x)$ is given by

$$P(Y=y|do(X=x))=\sum_zP(Y=y|X=x, Z=z)P(Z=z)$$
Where $Z$ is a variable set satisfying the backdoor criteria (A.K.A "adjustment set").

The ATE is given by:

\begin{equation}
  \begin{gathered}
\mathbb{E}(Y|do(X=x)) =\sum_y\sum_zyP(Y=y|X=x,Z=z)P(Z=z) = \\ \sum_zP(Z=z)\sum_yyP(Y=y|X=x,Z=z) = \sum_zP(Z=z)\mathbb{E}(Y|x,z)
  \end{gathered}
\end{equation}

We usually use regular machine learning framworks to estimate $\mathbb{E}(Y|x,z)$. When we predict for the original dataset we have the join probability of Z: $P(Z=z)$ preserved so no need to estimate it. I'll validate that in the results below.

Looking at the graph below: 

```{r plot graph again}
plot(graphLayout(g))
```

we can see that the adjustment set is $Z=\{Z\}$.

We'll use a linear regression to estimate $\mathbb{E}(Y|x,z)$.

Below are the estimated ATE using the correct procedure:

```{r estimate average treatment effect, results = "asis"}
model <- lm(Y ~ Z + X, data = sim_data)
Z_prob <- density(x = sim_data$Z)

interventions <- levels(sim_data$X)
ATE <- vector(length = length(interventions))
for(i in 1:length(interventions)){
  intervention_data <- data.frame(X = interventions[i], Z = Z_prob$x)
  ATE[i] <- sum(Z_prob$y*predict(model, intervention_data)/sum(Z_prob$y))
}

pandoc.table(data.frame(intervention = interventions, ATE = ATE))
```

Below are the ATE estimates when regressing Y on X:

```{r bi-variate regression, results = "asis"}
model <- lm(Y ~ X, data = sim_data)
ATE <- vector(length = length(interventions))
for(i in 1:length(interventions)){
  intervention_data <- data.frame(X = interventions[i])
  ATE[i] <- predict(model, intervention_data)
}
pandoc.table(data.frame(intervention = interventions, ATE = ATE))
```

We can see that the ordering is preserved but the estimates are all bundeled together.

Below are the ATE estimates when regressing Y on all variables:

```{r all variables regression, results = "asis"}
model <- lm(Y ~ ., data = sim_data)
ATE <- vector(length = length(interventions))
for(i in 1:length(interventions)){
  intervention_data <- sim_data %>% 
    mutate(X = interventions[i])
  ATE[i] <- mean(predict(model, intervention_data))
}
pandoc.table(data.frame(intervention = interventions, ATE = ATE))
```

We can see here X is estimated to have virtually no effect.

Below are the ATE estimates when using the correct adjustment set but without re-weighting:

```{r using correct adjustment set only, results = "asis"}
model <- lm(Y ~ X + Z, data = sim_data)
ATE <- vector(length = length(interventions))
for(i in 1:length(interventions)){
  intervention_data <- sim_data %>% 
    mutate(X = interventions[i])
  ATE[i] <- mean(predict(model, intervention_data))
}
pandoc.table(data.frame(intervention = interventions, ATE = ATE))
```
Looks like we're getting similar results! So no need to estimate density!

# Does the size of the adjustment set follows the same variance-bias trade off as in classic ML?

Let's look at the DAG below:

```{r define model 2}
g <- dagitty("dag {
Y [outcome]
X [exposure]
X -> Y [beta = 0.2]
Z1 -> X [beta = 0.1]
Z1 -> Z2 [beta = -0.1]
Z2 -> X [beta = 0.3]
Z2 -> Y [beta = 0.05]
Z3 -> Z2 [beta = -0.05]
Z3 -> Y [beta = -0.1]
}")

plot(graphLayout(g))
```

If we're interesting in estimating $\mathbb{E}(Y|do(X=x)$ there's several confounding paths that we need to block.

The exact possible adjustment sets are:

```{r print adjustment sets2}
M = 1000
print(adjustmentSets(g, type = "all"))
```

We can see that we can either control for 2 or 3 out of the confounding variables. Assuming we can measure them all, and we know from the graph they are all "relevant" (e.g. not white noise) does it make sense to use all 3 instead of just a subset?

Below I simulate `r M` dataset for a grid of sample size N and compare the standard deviation of each:

```{r simulate model 2 data}
N_grid <- seq(4, 40, 10)
model_sd <- data.frame(model = rep(c("small", "large"), length(N_grid)), 
                      N = rep(N_grid, each = 2), 
                      sd = NA)

for(n in N_grid){
  sim_data <- replicate(M, simulateSEM(g, N = n), simplify = F)
  model_sd$sd[model_sd$N == n & model_sd$model == "small"] <- sd(sapply(sim_data, function(data) coef(lm(Y ~ X + Z1 + Z2, data = data))[2]))
  model_sd$sd[model_sd$N == n & model_sd$model == "large"] <- sd(sapply(sim_data, function(data) coef(lm(Y ~ X + Z1 + Z2 + Z3, data = data))[2]))
}

```

Below are the sd results:

```{r print results}
model_sd %>% ggplot(aes(N, log(sd), color = model)) + geom_line()
```

So interestingly enough when we have a perfectly unbiased estimator it doesn't matter how many variables we're conditioning on.
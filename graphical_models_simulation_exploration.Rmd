---
title: "Graphical models simulation exploration"
author: "Iyar Lin"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  github_document:
    toc: true
    toc_depth: 2
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F, cache = F)
set.seed(1)
options(scipen = 999)

packages <- c(
  "devtools", # install_github
  "tidyverse", # best thing that happend to me
  "mgm",
  "pander", # table rendering
  "rpart"
)

sapply(
  packages,
  function(x) if (!require(x, character.only = TRUE, quietly = T)) {
      install.packages(x, quiet = T, verbose = F)
      library(x, character.only = T, quietly = T, verbose = F)
    }
)

if (!require(dagitty)) {
  install_github("jtextor/dagitty/r") # Analysis of structural causal models
  library(dagitty, quietly = T)
}
```

# Examples where having the model graph is crucial  
## Confounding variable example

Let's assume we measure the following variables:

* churn: Did a subscriber churn in the first 6 months? (Constructing it as a continuous variable for demonstration purposes)  
* cong: Network congestion (e.g. fill rate)  
* comp: Competition levels (e.g. distance to fiber)  

Let's also assume that the graph below depicts the causal relatioships between the above variables:

```{r define model1 and plot}
g <- dagitty("dag {
churn [outcome]
cong [exposure]
cong -> churn [beta = 0.15]
comp -> cong [beta = -0.4]
comp -> churn [beta = 0.9]
}")

plot(graphLayout(g))

N <- 100000
```

We'll also assume that the relationships are linear with the following coefficients:

$\beta_{cong\rightarrow churn} = 0.15$
$\beta_{comp\rightarrow cong} = -0.4$
$\beta_{comp\rightarrow churn} = 0.9$

The effect of increasing congestion on churn $\delta \text{churn} / \delta \text{cong}$ is:

$$\delta \text{churn} / \delta \text{cong} = \beta_{cong\rightarrow churn} = 0.15$$

Below I simulate a dataset from the above graph with `r N` observations

```{r simulate dataset}
sim_data <- simulateSEM(g, N = N, standardized = T)
```

If we were to plot the relation between congestion and churn we would get:

```{r plot bi-variate relation}
sim_data %>%
  ggplot(aes(cong, churn)) +
  geom_point(alpha = 0.05) + geom_smooth(method = "lm")
```

Fitting the linear model:

$$churn = \beta_1cong + \epsilon$$

```{r fit model1}
model <- lm(churn ~ cong, data = sim_data)
```

We get the following coefficients:

```{r model coefficients, results = "asis"}
pandoc.table(coefficients(model))
```

We get the opposite relationship than what we'd expect!
Notionaly we know that may arise due to competition being a confounding variable. We usually fix confounding variables by "controlling" for as many things we can.

If we fit the model:

$$churn = \beta_1cong + \beta_2comp + \epsilon$$

```{r fit model2}
model <- lm(churn ~ cong + comp, data = sim_data)
```

We get the following coefficients:

```{r model coefficients2, results = "asis"}
pandoc.table(coefficients(model))
```

Obtaining the true congestion effect.

## Blocking variable

Next let's assume we also measure:  

* CX: Customer experience (e.g. buffering events/calls to care)  


We'll assume the graph below depicts the causal relatioships between the all measured variables:

```{r define model 2}
g <- dagitty("dag {
churn [outcome]
cong [exposure]
cong -> CX [beta = -0.3]
CX -> churn [beta = -0.5]
comp -> cong [beta = -0.4]
comp -> churn [beta = 0.9]
}")

plot(graphLayout(g))
```

We'll also assume that the relationships are linear with the following coefficients:

$\beta_{cong\rightarrow CX} = -0.3$
$\beta_{CX\rightarrow churn} = -0.5$
$\beta_{comp\rightarrow cong} = -0.4$
$\beta_{comp\rightarrow churn} = 0.9$

The effect of increasing congestion on churn $\delta \text{churn} / \delta \text{cong}$ in this case is:

$$\delta \text{churn} / \delta \text{cong} = \beta_{cong\rightarrow CX} \cdot \beta_{CX\rightarrow churn} = 0.15$$

This time around if we use all the measures variables in our regression: 

$$churn = \beta_1cong + \beta_2comp + \beta_3CX + \epsilon$$
```{r fit model3, results = "asis"}
sim_data <- simulateSEM(g, N = N, standardized = T)
model <- lm(churn ~ ., data = sim_data)
pandoc.table(coefficients(model))
```

We can see that the coefficient of congestion is about 0. This happens because conditioning CX blocks the path from cong to churn.

If we instead fit the model:

$$churn = \beta_1cong + \beta_2comp + \epsilon$$

```{r fit model4}
model <- lm(churn ~ cong + comp, data = sim_data)
```
We get the following coefficients:

```{r model coefficients3, results = "asis"}
pandoc.table(coefficients(model))
```
Obtaining the true congestion effect.

## Collider nodes (advanced)

Next let's assume we also measure:  

* brand: Our company's brand (e.g. we suck, we're the best in the market)  
* demo: Demographics (rich population, dense population etc)  

We'll assume the graph below depicts the causal relatioships between the all measured variables:

```{r define model 3}
g <- dagitty("dag {
churn [outcome]
cong [exposure]
cong -> CX [beta = -0.3]
CX -> churn [beta = -0.5]
comp -> cong [beta = -0.4]
comp -> churn [beta = 0.9]
cong -> brand [beta = -0.8]
brand -> comp [beta = 0.7]
demo -> comp [beta = 0.4]
demo -> churn [beta = -0.7]
}")

plot(graphLayout(g))
```

This time around neither conditioning on comp neither using all variables will help us. We'll have to use grpahical model math to discover that we need to condition on: $\{comp, demo\}$.

So running the model:

$$churn = \beta_1cong + \beta_2comp + \beta_3 demo + \epsilon$$

```{r fit model5}
sim_data <- simulateSEM(g, N = N, standardized = T)
model <- lm(churn ~ cong + comp + demo, data = sim_data)
```
We get the following coefficients:

```{r model coefficients4, results = "asis"}
pandoc.table(coefficients(model))
```

Obtaining the true congestion effect.


# Estimate model structure

R has a package I dont understand yet that enables infering the model "skeleton" based on the data and an assumption about the maximum possible interaction depth. 

```{r find graph skeleton}
mfit <- mgm(sim_data, type = rep("g", ncol(sim_data)), level = rep(1, ncol(sim_data)), k = 3, verbatim = T)
FactorGraph(mfit, labels = names(sim_data))
```

We can see that we're able to find the skeleton correctly.

